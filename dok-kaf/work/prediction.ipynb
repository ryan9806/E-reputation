{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1a29cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 1.7 MB/s eta 0:00:01     |█████████████▋                  | 634 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.60.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.3.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4473571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.types import IntegerType, ArrayType, BooleanType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer,NGram,HashingTF,IDF\n",
    "from pyspark.sql.functions import concat,col\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit,CrossValidator\n",
    "import nltk.corpus \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68728c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancement de la session spark\n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .appName('twitter')\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91744d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation de la df a partir du csv twitter\n",
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Twitter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94cd8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nettoyage de la df\n",
    "dfn=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e87907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation de la colonne Note\n",
    "df1 = dfn.withColumn(\"Note\", when(dfn.sentiment ==\"Positive\" ,1)\n",
    "                                                    .when(dfn.sentiment==\"Negative\" ,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c01c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----+--------------------+\n",
      "|Tweet ID|     entity|sentiment|       Tweet content|Note|               words|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|\n",
      "|    2401|Borderlands| Positive|I am coming to th...|   1|[i, am, coming, t...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|   1|[im, coming, on, ...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|\n",
      "|    2401|Borderlands| Positive|im getting into b...|   1|[im, getting, int...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|   1|[2010, so, i, spe...|\n",
      "|    2402|Borderlands| Positive|                 was|   1|               [was]|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2404|Borderlands| Positive|this was the firs...|   1|[this, was, the, ...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2404|Borderlands| Positive|that I was the fi...|   1|[that, i, was, th...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|\n",
      "|    2405|Borderlands| Negative|the biggest dissa...|   0|[the, biggest, di...|\n",
      "|    2405|Borderlands| Negative|The biggest disap...|   0|[the, biggest, di...|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tokenzed\n",
    "tokenizer = Tokenizer(inputCol=\"Tweet content\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(df1)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb2bd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Telechargement des stop word de la bib nltk\n",
    "nltk.download('stopwords')\n",
    "add_stopwords= stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2d1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+----+--------------------+--------------------+\n",
      "|Tweet ID|     entity|sentiment|       Tweet content|Note|               words|            Resultat|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+--------------------+\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|[im, getting, bor...|\n",
      "|    2401|Borderlands| Positive|I am coming to th...|   1|[i, am, coming, t...|[coming, borders,...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|[im, getting, bor...|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|   1|[im, coming, on, ...|[im, coming, bord...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|   1|[im, getting, on,...|[im, getting, bor...|\n",
      "|    2401|Borderlands| Positive|im getting into b...|   1|[im, getting, int...|[im, getting, bor...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|[spent, hours, ma...|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|   1|[so, i, spent, a,...|[spent, couple, h...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|[spent, hours, so...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|   1|[so, i, spent, a,...|[spent, hours, ma...|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|   1|[2010, so, i, spe...|[2010, spent, hou...|\n",
      "|    2402|Borderlands| Positive|                 was|   1|               [was]|                  []|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|this was the firs...|   1|[this, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2404|Borderlands| Positive|that I was the fi...|   1|[that, i, was, th...|[first, real, bor...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|   1|[that, was, the, ...|[first, borderlan...|\n",
      "|    2405|Borderlands| Negative|the biggest dissa...|   0|[the, biggest, di...|[biggest, dissapp...|\n",
      "|    2405|Borderlands| Negative|The biggest disap...|   0|[the, biggest, di...|[biggest, disappo...|\n",
      "+--------+-----------+---------+--------------------+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Stop Words\n",
    "remover = StopWordsRemover(stopWords=add_stopwords)\n",
    "remover.setInputCol(\"words\")\n",
    "remover.setOutputCol(\"Resultat\")\n",
    "df= remover.transform(tokenized)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017619f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ngram\n",
    "ngram = NGram(n=2)\n",
    "ngram.setInputCol(\"Resultat\")\n",
    "ngram.setOutputCol(\"Ngram\")\n",
    "ngramDataFrame = ngram.transform(df)\n",
    "ngramDataFrame=ngramDataFrame.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed69955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashing\n",
    "hashingTF = HashingTF(inputCol=\"Ngram\", outputCol=\"features\")\n",
    "hashing=hashingTF.transform(ngramDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba465b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashing)\n",
    "df_idf = idfModel.transform(hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d2e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='idf',\n",
    "    labelCol='Note')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fe00ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split sur la df\n",
    "train,test=df_idf.randomSplit([0.8,0.2],seed=12345)\n",
    "lrModel = lr.fit(train)\n",
    "out=lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2530247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.878396819620521"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BinaryClassificationEvaluator\n",
    "evaluator2 = BinaryClassificationEvaluator(labelCol=\"Note\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "evaluator2.evaluate(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44a1c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation de la pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,ngram, hashingTF, idf , lr])\n",
    "#entrainement de la pip \n",
    "model_pip = pipeline.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0070287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save de la pipeline\n",
    "model_pip.write().overwrite().save(\"model_pip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
