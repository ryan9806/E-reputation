{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61d3f41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.8/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02476e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.8/site-packages (4.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4952030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode\n",
    "from kafka import KafkaConsumer\n",
    "import copy\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField,DateType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac8436f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancemement de la session spark\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('twitter')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\n",
    "         .config(\"spark.mongodb.input.uri\", \"mongodb://mongo:27017/db_Booking.*\")\n",
    "         .config(\"spark.mongodb.output.uri\", \"mongodb://mongo:27017/db_Booking.*\")\n",
    "         .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97680311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation du la connexion mongo\n",
    "client = MongoClient('mongo',27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82c9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load de la pipeline saved dans prediction.py\n",
    "pipe = PipelineModel.load(\"model_pip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1166bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df pour recuperer le schema de la df final\n",
    "df_sc = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"Twittersc.csv\")\n",
    "# df_sc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef9c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#application de la pipeline sur la df\n",
    "df_sc = pipe.transform(df_sc)\n",
    "#organisation du schema\n",
    "df_sc = df_sc.withColumnRenamed(\"sentiment\",\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4916ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copie du schema de la df de test\n",
    "s=copy.deepcopy(df_sc.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c01f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation de la df final avec le meme schema de fin\n",
    "df_final = spark.createDataFrame([], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a408fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialistaion du kafkaConsumer\n",
    "c = KafkaConsumer('test5', bootstrap_servers=['kafka:9093'], api_version=(2,6,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34fb32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation du schema de recuperation des messages \n",
    "cSchema = StructType([StructField(\"Tweet content\", StringType())\\\n",
    "                     ,StructField(\"date\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6e3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction inserer csv qui sert a append les messages(tweet) recu par le produceur dans un fichier csv\n",
    "def csv_insert(df,f_name) :\n",
    "    #df spark to df pandas\n",
    "    df = df_final_csv.toPandas()\n",
    "    #df pandas to dict\n",
    "    mon_dict=df.to_dict(orient=\"record\")\n",
    "    #si le fichier existe on le cree\n",
    "    if os. path.exists(f_name) :\n",
    "        with open(f_name, 'a') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames= mon_dict[len(mon_dict)-1].keys())\n",
    "            writer.writerow(mon_dict[len(mon_dict)-1])\n",
    "    #sinon on append direct sans creer de nouveau fichier\n",
    "    else:\n",
    "        with open(f_name, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=mon_dict[len(mon_dict)-1].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerow(mon_dict[len(mon_dict)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7961c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       Tweet content|      date|               words|            Resultat|               Ngram|            features|                 idf|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|[rt, @icscert:, ?...|[rt, @icscert:, ?...|[rt @icscert:, @i...|(262144,[20800,25...|(262144,[20800,25...|[9.59890765140024...|[0.99993220183659...|       0.0|\n",
      "|Baxter Healthcare...|08/02/2022|[baxter, healthca...|[baxter, healthca...|[baxter healthcar...|(262144,[723,2080...|(262144,[723,2080...|[17.0373785398440...|[0.99999996011950...|       0.0|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+----------+---------+\n",
      "|       Tweet content|      date|prediction|Sentiment|\n",
      "+--------------------+----------+----------+---------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|       0.0| Negative|\n",
      "|Baxter Healthcare...|08/02/2022|       0.0| Negative|\n",
      "+--------------------+----------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:1549: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       Tweet content|      date|               words|            Resultat|               Ngram|            features|                 idf|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|[rt, @icscert:, ?...|[rt, @icscert:, ?...|[rt @icscert:, @i...|(262144,[20800,25...|(262144,[20800,25...|[9.59890765140024...|[0.99993220183659...|       0.0|\n",
      "|Baxter Healthcare...|08/02/2022|[baxter, healthca...|[baxter, healthca...|[baxter healthcar...|(262144,[723,2080...|(262144,[723,2080...|[17.0373785398440...|[0.99999996011950...|       0.0|\n",
      "|Minutes from the ...|07/02/2022|[minutes, from, t...|[minutes, cve, bo...|[minutes cve, cve...|(262144,[12771,64...|(262144,[12771,64...|[28.2402923969515...|[0.99999999999945...|       0.0|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+----------+---------+\n",
      "|       Tweet content|      date|prediction|Sentiment|\n",
      "+--------------------+----------+----------+---------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|       0.0| Negative|\n",
      "|Baxter Healthcare...|08/02/2022|       0.0| Negative|\n",
      "|Minutes from the ...|07/02/2022|       0.0| Negative|\n",
      "+--------------------+----------+----------+---------+\n",
      "\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       Tweet content|      date|               words|            Resultat|               Ngram|            features|                 idf|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|[rt, @icscert:, ?...|[rt, @icscert:, ?...|[rt @icscert:, @i...|(262144,[20800,25...|(262144,[20800,25...|[9.59890765140024...|[0.99993220183659...|       0.0|\n",
      "|Baxter Healthcare...|08/02/2022|[baxter, healthca...|[baxter, healthca...|[baxter healthcar...|(262144,[723,2080...|(262144,[723,2080...|[17.0373785398440...|[0.99999996011950...|       0.0|\n",
      "|Minutes from the ...|07/02/2022|[minutes, from, t...|[minutes, cve, bo...|[minutes cve, cve...|(262144,[12771,64...|(262144,[12771,64...|[28.2402923969515...|[0.99999999999945...|       0.0|\n",
      "|584 CVE Records +...|06/02/2022|[584, cve, record...|[584, cve, record...|[584 cve, cve rec...|(262144,[7531,584...|(262144,[7531,584...|[1.67960311056601...|[0.84285196925901...|       0.0|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+----------+---------+\n",
      "|       Tweet content|      date|prediction|Sentiment|\n",
      "+--------------------+----------+----------+---------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|       0.0| Negative|\n",
      "|Baxter Healthcare...|08/02/2022|       0.0| Negative|\n",
      "|Minutes from the ...|07/02/2022|       0.0| Negative|\n",
      "|584 CVE Records +...|06/02/2022|       0.0| Negative|\n",
      "+--------------------+----------+----------+---------+\n",
      "\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       Tweet content|      date|               words|            Resultat|               Ngram|            features|                 idf|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|[rt, @icscert:, ?...|[rt, @icscert:, ?...|[rt @icscert:, @i...|(262144,[20800,25...|(262144,[20800,25...|[9.59890765140024...|[0.99993220183659...|       0.0|\n",
      "|Baxter Healthcare...|08/02/2022|[baxter, healthca...|[baxter, healthca...|[baxter healthcar...|(262144,[723,2080...|(262144,[723,2080...|[17.0373785398440...|[0.99999996011950...|       0.0|\n",
      "|Minutes from the ...|07/02/2022|[minutes, from, t...|[minutes, cve, bo...|[minutes cve, cve...|(262144,[12771,64...|(262144,[12771,64...|[28.2402923969515...|[0.99999999999945...|       0.0|\n",
      "|584 CVE Records +...|06/02/2022|[584, cve, record...|[584, cve, record...|[584 cve, cve rec...|(262144,[7531,584...|(262144,[7531,584...|[1.67960311056601...|[0.84285196925901...|       0.0|\n",
      "|Dutch Institute f...|01/02/2022|[dutch, institute...|[dutch, institute...|[dutch institute,...|(262144,[11138,38...|(262144,[11138,38...|[-0.4978875573645...|[0.37803722892892...|       1.0|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+----------+---------+\n",
      "|       Tweet content|      date|prediction|Sentiment|\n",
      "+--------------------+----------+----------+---------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|       0.0| Negative|\n",
      "|Baxter Healthcare...|08/02/2022|       0.0| Negative|\n",
      "|Minutes from the ...|07/02/2022|       0.0| Negative|\n",
      "|584 CVE Records +...|06/02/2022|       0.0| Negative|\n",
      "|Dutch Institute f...|01/02/2022|       1.0| Positive|\n",
      "+--------------------+----------+----------+---------+\n",
      "\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       Tweet content|      date|               words|            Resultat|               Ngram|            features|                 idf|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|[rt, @icscert:, ?...|[rt, @icscert:, ?...|[rt @icscert:, @i...|(262144,[20800,25...|(262144,[20800,25...|[9.59890765140024...|[0.99993220183659...|       0.0|\n",
      "|Baxter Healthcare...|08/02/2022|[baxter, healthca...|[baxter, healthca...|[baxter healthcar...|(262144,[723,2080...|(262144,[723,2080...|[17.0373785398440...|[0.99999996011950...|       0.0|\n",
      "|Minutes from the ...|07/02/2022|[minutes, from, t...|[minutes, cve, bo...|[minutes cve, cve...|(262144,[12771,64...|(262144,[12771,64...|[28.2402923969515...|[0.99999999999945...|       0.0|\n",
      "|584 CVE Records +...|06/02/2022|[584, cve, record...|[584, cve, record...|[584 cve, cve rec...|(262144,[7531,584...|(262144,[7531,584...|[1.67960311056601...|[0.84285196925901...|       0.0|\n",
      "|Dutch Institute f...|01/02/2022|[dutch, institute...|[dutch, institute...|[dutch institute,...|(262144,[11138,38...|(262144,[11138,38...|[-0.4978875573645...|[0.37803722892892...|       1.0|\n",
      "|A BIG WELCOME to ...|01/02/2022|[a, big, welcome,...|[big, welcome, ne...|[big welcome, wel...|(262144,[30436,41...|(262144,[30436,41...|[-0.6901039319272...|[0.33400995347228...|       1.0|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+----------+---------+\n",
      "|       Tweet content|      date|prediction|Sentiment|\n",
      "+--------------------+----------+----------+---------+\n",
      "|RT @ICSCERT: ðŸŽ‰ @...|09/02/2022|       0.0| Negative|\n",
      "|Baxter Healthcare...|08/02/2022|       0.0| Negative|\n",
      "|Minutes from the ...|07/02/2022|       0.0| Negative|\n",
      "|584 CVE Records +...|06/02/2022|       0.0| Negative|\n",
      "|Dutch Institute f...|01/02/2022|       1.0| Positive|\n",
      "|A BIG WELCOME to ...|01/02/2022|       1.0| Positive|\n",
      "+--------------------+----------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'streamlit.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f4c9ba177f4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdf_final_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uri\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mongodb://mongo:27017/Twitter.sentiment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#sauvgarde des donnees dans un csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcsv_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_final_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'streamlit.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7fa91fcf6582>\u001b[0m in \u001b[0;36mcsv_insert\u001b[0;34m(df, f_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#si le fichier existe on le cree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmon_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmon_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmon_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmon_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'streamlit.csv'"
     ]
    }
   ],
   "source": [
    "#recuperation des messages envoyÃ©s par le produceur\n",
    "for msg in c:\n",
    "    #message to df pandas\n",
    "    df = pd.DataFrame(json.loads(msg.value))\n",
    "    #df pandas to df spark\n",
    "    dfs = spark.createDataFrame(df,schema=cSchema)\n",
    "    #tranformation avec la pipe\n",
    "    df_pipe = pipe.transform(dfs)\n",
    "    #construction de la df final avec une union des df tranformÃ© a chaque message\n",
    "    df_final=df_final.union(df_pipe)\n",
    "    df_final.show()\n",
    "    #nettoyage de la df final\n",
    "    df_final_csv= df_final.drop('words','Resultat','Ngram','features','idf','rawPrediction','probability')\n",
    "    #creation de la colonne sentimenet pour faciliter la visualisation derreire\n",
    "    df_final_csv = df_final_csv.withColumn(\"Sentiment\", when(df_final_csv.prediction == 1,\"Positive\")\n",
    "                                                    .when(df_final_csv.prediction== 0 ,\"Negative\"))\n",
    "    df_final_csv.show()\n",
    "    #Creation et insertion dans la bdd mongo\n",
    "    df_final_csv.write.format(\"mongo\").mode(\"append\").option(\"uri\",\"mongodb://mongo:27017/Twitter.sentiment\").save()\n",
    "    #sauvgarde des donnees dans un csv\n",
    "    csv_insert(df_final_csv,'streamlit.csv')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
